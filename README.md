# Temba-Digital-Bridge_WASH_AI-Assistant-Chatbot
A domain-specific generative AI assistant designed to provide intelligent, step-by-step guidance on Water, Sanitation, and Hygiene (WASH) for community health.
<div align="center">

<!-- HEADER BANNER -->
<img src="https://capsule-render.vercel.app/api?type=waving&color=0077B6,00B4D8,90E0EF&height=220&section=header&text=Temba%20Digital%20Bridge&fontSize=52&fontColor=ffffff&fontAlignY=38&desc=WASH%20AI%20Assistant%20%7C%20QLoRA%20%2B%20LoRA%20Fine-Tuned%20LLM%20for%20Water%2C%20Sanitation%20%26%20Public%20Health&descSize=14&descAlignY=60" width="100%"/>

An AI-powered chatbot providing guidance on **Water, Sanitation, and Hygiene (WASH)** topics. Built to support communities with accessible, reliable information on water safety, sanitation practices, hygiene, and waterborne disease prevention.

---

## ğŸš€ Live Application

> ### ğŸ‘‰ [**Try Temba Digital Bridge Live â†’**](https://huggingface.co/spaces/Fidele-Ndihokubwayo/Temba_Digital_Bridge_AI_Assistant_Chatbot)
> 
> **https://huggingface.co/spaces/Fidele-Ndihokubwayo/Temba_Digital_Bridge_AI_Assistant_Chatbot**

---

## ğŸ’¡ What Can Temba Help With?

- ğŸš° **Water Safety** â€” How to treat, purify, and safely store drinking water
- ğŸ§¼ **Hygiene & Sanitation** â€” Handwashing, waste disposal, and best practices
- ğŸ¦  **Waterborne Diseases** â€” Symptoms, prevention, and treatment of cholera, typhoid, diarrhoea, and more
- ğŸ—ï¸ **Infrastructure Guidance** â€” Wells, boreholes, pipelines, and water storage

---
<br/>

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-Transformers-FFD21F?style=for-the-badge)](https://huggingface.co)
[![PEFT](https://img.shields.io/badge/PEFT-QLoRA%20%2B%20LoRA-8B5CF6?style=for-the-badge)](https://github.com/huggingface/peft)
[![Gradio](https://img.shields.io/badge/Gradio-4.0+-FF7C00?style=for-the-badge&logo=gradio&logoColor=white)](https://gradio.app)
[![Colab](https://img.shields.io/badge/Google%20Colab-T4%20GPU-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com)
[![SDG6](https://img.shields.io/badge/UN%20SDG%206-Clean%20Water%20%26%20Sanitation-26BDE2?style=for-the-badge)](https://sdgs.un.org/goals/goal6)
[![License](https://img.shields.io/badge/License-Educational%20Use-22C55E?style=for-the-badge)](LICENSE)

<br/>

<table>
<tr>
<td align="center"><b>ğŸ§  Base Model</b><br/>TinyLlama-1.1B-Chat</td>
<td align="center"><b>âš¡ Fine-Tuning</b><br/>QLoRA + LoRA</td>
<td align="center"><b>ğŸ’¾ VRAM Usage</b><br/>2.28 GB Peak</td>
<td align="center"><b>â±ï¸ Train Time</b><br/>~25 min on T4</td>
<td align="center"><b>ğŸ›¡ï¸ OOD Refusal</b><br/>100% Success</td>
<td align="center"><b>ğŸ“Š Experiments</b><br/>5 Controlled</td>
</tr>
</table>

<br/>

> ### ğŸ’§ *"Having a water point should mean having safe, sustainable water for all."*
>
> A domain-specific generative AI assistant fine-tuned for Water, Sanitation & Public Health (WASH) â€”
> built under real-world resource constraints, deployed for maximum community impact.

<br/>

**Holistic CleanFlow | Temba Digital Bridge Initiative**
<br/>
*Transforming water management from a static infrastructure model into an intelligent, AI-powered service-delivery ecosystem.*

</div>

---

## ğŸ“‹ Table of Contents

<details open>
<summary><b>Click to expand / collapse</b></summary>

| # | Section |
|---|---------|
| 1 | [Project Overview](#1-project-overview) |
| 2 | [Problem Statement](#2-problem-statement) |
| 3 | [Proposed Solution](#3-proposed-solution) |
| 4 | [Dataset Collection & Curation](#4-dataset-collection--curation) |
| 5 | [Preprocessing Pipeline](#5-preprocessing-pipeline) |
| 6 | [Model Architecture](#6-model-architecture) |
| 7 | [Fine-Tuning Strategy â€” QLoRA + LoRA](#7-fine-tuning-strategy--qlora--lora) |
| 8 | [Experimental Framework](#8-experimental-framework) |
| 9 | [Evaluation Metrics](#9-evaluation-metrics) |
| 10 | [Results & Analysis](#10-results--analysis) |
| 11 | [Domain Boundary Handling](#11-domain-boundary-handling) |
| 12 | [User Interface (Gradio)](#12-user-interface-gradio) |
| 13 | [Notebook Structure](#13-notebook-structure) |
| 14 | [Architecture Table](#14-architecture-table) |
| 15 | [How to Run](#15-how-to-run) |
| 16 | [Dependencies](#16-dependencies) |
| 17 | [Rubric Coverage Map](#17-rubric-coverage-map) |
| 18 | [Conclusion](#18-conclusion) |
| 19 | [References & Acknowledgements](#19-references--acknowledgements) |

</details>

---

## 1. Project Overview

The **Temba Digital Bridge AI Assistant** is a domain-specific generative chatbot fine-tuned from the [`TinyLlama-1.1B-Chat-v1.0`](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) base model using **QLoRA** (4-bit quantization) and **LoRA** (Low-Rank Adaptation) â€” two of the most powerful parameter-efficient fine-tuning techniques available for large language models operating under real-world compute constraints.

The assistant is not a general-purpose chatbot. It is deliberately and precisely scoped to serve communities and service providers operating in water-scarce, sanitation-deficient environments. It functions as an always-on, expert-level WASH advisor â€” capable of providing step-by-step guidance on water treatment, infrastructure repair, disease prevention, and clinical health management â€” all without requiring an internet connection to a centralised server or access to a human specialist.

The system specialises **exclusively** in four interconnected WASH domains:

<div align="center">

| Domain | Coverage Area |
|--------|--------------|
| ğŸ’§ **Water Safety** | Purification methods, chlorination dosing, contamination detection, turbidity assessment, safe storage |
| ğŸ§¼ **Sanitation & Hygiene** | Handwashing protocols, latrine construction & maintenance, waste disposal, household hygiene practices |
| ğŸ”§ **Infrastructure Maintenance** | Borehole repair, handpump diagnostics, piping, valve maintenance, storage tank management |
| ğŸ¦  **Public Health** | Cholera, typhoid, dysentery, diarrhoeal disease, ORS preparation, dehydration management, outbreak prevention |

</div>

> **ğŸ›¡ï¸ Safety by Design:** The system enforces strict domain boundary handling. When a user submits a question outside these four domains, the model returns a predefined, professional refusal message â€” rather than hallucinating a plausible-sounding but potentially dangerous answer. This is not a limitation; it is a deliberate safety feature for health-critical deployment.

### âš™ï¸ Technical Snapshot

| Attribute | Detail |
|-----------|--------|
| **Base Model** | `TinyLlama/TinyLlama-1.1B-Chat-v1.0` |
| **Fine-Tuning Method** | QLoRA (4-bit NF4) + LoRA Adapters |
| **Target Modules** | `q_proj`, `v_proj` |
| **LoRA Rank** | 16 (Experiments 1, 2, 4, 5) / 8 (Experiment 3) |
| **Dataset Size (post-filtering)** | â‰¥ 1,000 WASH-aligned samples |
| **Training Environment** | Google Colab (T4 GPU, 15 GB VRAM) |
| **Peak VRAM Usage** | 2.28 GB |
| **Training Duration** | ~24.8 minutes (Experiment 1) |
| **UI Framework** | Gradio |
| **Global Seed** | 42 |

---

## 2. Problem Statement

### 2.1 The Core Crisis

Access to clean water and proper sanitation is not merely a comfort â€” it is a fundamental determinant of human survival, dignity, and economic development. Yet across sub-Saharan Africa, South Asia, and other water-stressed regions, millions of people live within reach of water infrastructure that they cannot effectively use, maintain, or trust. The problem is not simply the absence of boreholes, handpumps, and water treatment facilities. The problem is the systematic absence of reliable knowledge to operate them correctly.

Communities in water-scarce and sanitation-deficient regions face a critical and widening expertise gap: **infrastructure exists, but the knowledge to use it safely does not.** This is one of the defining public health failures of our era â€” entirely preventable, yet persistently unaddressed.

### 2.2 Four Compounding Systemic Failures

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SYSTEMIC FAILURE CHAIN                               â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                                   â”‚
â”‚  ğŸ’§ BROKEN INFRASTRUCTUREâ”‚  Boreholes and handpumps break down regularly    â”‚
â”‚                          â”‚  due to mechanical wear, sediment buildup, or    â”‚
â”‚                          â”‚  poor installation. Without immediate technical  â”‚
â”‚                          â”‚  guidance, they remain unrepaired for weeks or   â”‚
â”‚                          â”‚  months â€” forcing communities back to unsafe     â”‚
â”‚                          â”‚  surface water sources like rivers and ponds.    â”‚
â”‚                          â”‚                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                                   â”‚
â”‚  ğŸ¦  WATERBORNE DISEASE   â”‚  Cholera, typhoid, and dysentery are not         â”‚
â”‚     OUTBREAKS            â”‚  inevitable â€” they are predictable consequences  â”‚
â”‚                          â”‚  of improper water purification, poor latrine    â”‚
â”‚                          â”‚  management, and inadequate hygiene practices.   â”‚
â”‚                          â”‚  A single contaminated water source can kill     â”‚
â”‚                          â”‚  dozens within days without early intervention.  â”‚
â”‚                          â”‚                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                                   â”‚
â”‚  ğŸ• NO 24/7 ACCESS       â”‚  Water safety specialists, public health nurses, â”‚
â”‚     TO EXPERTISE         â”‚  and infrastructure engineers are scarce and     â”‚
â”‚                          â”‚  concentrated in urban centres. Rural            â”‚
â”‚                          â”‚  communities â€” who need guidance most urgently   â”‚
â”‚                          â”‚  â€” have no mechanism to access expert advice     â”‚
â”‚                          â”‚  when a crisis begins at 2:00 AM.               â”‚
â”‚                          â”‚                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                                   â”‚
â”‚  ğŸ“¡ COMMUNICATION GAP    â”‚  Water service providers â€” government agencies,  â”‚
â”‚                          â”‚  NGOs, utility companies â€” have no scalable      â”‚
â”‚                          â”‚  channel to relay safety updates, boil-water     â”‚
â”‚                          â”‚  advisories, or maintenance instructions to the  â”‚
â”‚                          â”‚  communities they serve. Information moves        â”‚
â”‚                          â”‚  slowly, inconsistently, and often too late.     â”‚
â”‚                          â”‚                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Human Cost & SDG Alignment

These four failures do not exist in isolation â€” they compound one another in a destructive cycle. A broken handpump forces a family to collect water from an unsafe surface source. That water contains pathogens. Without guidance on purification, the family drinks it. Children contract diarrhoeal disease. Without accessible ORS preparation guidance, dehydration becomes severe. The local health post is overwhelmed and understaffed. The community loses trust in both infrastructure and institutions.

These failures translate directly into:

- **Preventable deaths** â€” particularly among children under five, where diarrheal diseases remain a leading killer globally
- **Economic loss** â€” families spending productive hours fetching water from distant sources, or days sick from waterborne illness
- **Deteriorating public health** â€” cyclical disease outbreaks that overwhelm underfunded rural health systems
- **Erosion of community trust** â€” in both the physical infrastructure and the institutions responsible for maintaining it

All of this occurs in communities where **UN Sustainable Development Goal 6** â€” *"Ensure availability and sustainable management of water and sanitation for all"* â€” remains critically unmet. Achieving SDG 6 requires not just building infrastructure, but ensuring that communities have the knowledge and tools to use it safely and sustainably. The Temba Digital Bridge is designed to close that knowledge gap.

---

## 3. Proposed Solution

### 3.1 Vision

The Temba Digital Bridge AI Assistant is designed around a single, urgent insight: **the knowledge required to save lives in a water crisis already exists â€” it simply cannot reach the people who need it in time.** The solution is not to create new knowledge. It is to compress decades of accumulated WASH expertise into a lightweight, always-available, conversational AI system that can operate on low-resource hardware, in low-bandwidth environments, and in the hands of community health workers who are not engineers or clinicians.

### 3.2 Five Core Design Principles

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                        TEMBA SOLUTION FRAMEWORK                            â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                                                            â”‚
  â”‚  ğŸŒ  DEMOCRATISES EXPERTISE                                                â”‚
  â”‚      Provides 24/7 step-by-step technical and clinical guidance on water  â”‚
  â”‚      purification, borehole maintenance, disease prevention, and ORS       â”‚
  â”‚      preparation â€” to anyone, anywhere, on any device.                    â”‚
  â”‚                                                                            â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                                                            â”‚
  â”‚  ğŸ”—  BRIDGES COMMUNICATION                                                 â”‚
  â”‚      Acts as a real-time link between communities and water service        â”‚
  â”‚      providers. Field workers, community health volunteers, and household  â”‚
  â”‚      users can receive accurate, actionable guidance without waiting for   â”‚
  â”‚      a technician to arrive.                                               â”‚
  â”‚                                                                            â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                                                            â”‚
  â”‚  ğŸ›¡ï¸  ENFORCES SAFETY THROUGH DOMAIN BOUNDARIES                            â”‚
  â”‚      Rather than attempting to answer everything and risking dangerous      â”‚
  â”‚      hallucinations, the assistant refuses out-of-domain queries           â”‚
  â”‚      completely. In health-sensitive contexts, a wrong answer is worse     â”‚
  â”‚      than no answer. The refusal mechanism is a feature, not a limitation. â”‚
  â”‚                                                                            â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                                                            â”‚
  â”‚  âš¡  OPERATES UNDER REAL-WORLD CONSTRAINTS                                 â”‚
  â”‚      Deployed with 4-bit quantization and LoRA adapters, the model runs    â”‚
  â”‚      at 2.28 GB peak VRAM â€” compatible with mobile devices, web            â”‚
  â”‚      interfaces, and low-bandwidth community deployments. It was trained   â”‚
  â”‚      entirely on a free-tier Google Colab T4 GPU in under 25 minutes.     â”‚
  â”‚                                                                            â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                                                            â”‚
  â”‚  ğŸ¯  ALIGNS DIRECTLY WITH UN SDG 6                                         â”‚
  â”‚      The assistant embodies the principle that "having a water point"       â”‚
  â”‚      must truly mean "having safe, sustainable water." Infrastructure       â”‚
  â”‚      without knowledge is incomplete. This system provides the knowledge   â”‚
  â”‚      layer that makes infrastructure meaningful.                           â”‚
  â”‚                                                                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 What Makes This Approach Unique

Unlike deploying a generic large language model (such as GPT-4 or Llama-2 in its base form), the Temba Digital Bridge is purpose-built through domain-specific fine-tuning. This matters for three critical reasons:

1. **Accuracy** â€” A model trained specifically on WASH data produces more precise, contextually appropriate guidance than a general model attempting to retrieve relevant knowledge from billions of parameters tuned for a completely different purpose.
2. **Safety** â€” Domain restriction means the model cannot be accidentally used for unrelated purposes in ways that might cause harm in a health-critical setting. The domain gate is not just filtering â€” it is a safety contract.
3. **Efficiency** â€” A 1.1B parameter model with LoRA adapters is orders of magnitude cheaper to deploy and maintain than a frontier model API, making it viable for organisations with limited infrastructure budgets operating in low-connectivity regions.

---

## 4. Dataset Collection & Curation

### 4.1 Strategic Multi-Source Corpus Design

Rather than training on a single generic dataset â€” which would either lack domain specificity or fail to produce conversationally fluent responses â€” a **hybrid domain corpus** was carefully constructed from three strategically selected sources, each contributing a distinct and non-redundant capability to the final model.

The guiding principle was **triangulation**: cover the clinical authority needed to handle health emergencies, the technical precision needed to guide infrastructure troubleshooting, and the conversational robustness needed to interpret the wide variety of ways real users phrase their questions.

| Dataset Source | Domain Focus | Samples Selected | Original Size | Purpose |
|----------------|--------------|:---------------:|:-------------:|---------|
| **MedAlpaca Medical Flashcards** | Clinical Health | 1,500 | 33,955 | Waterborne disease identification, ORS guidance, symptom-response pairs for cholera, typhoid, dysentery |
| **SQuAD v2 (Filtered)** | WASH Infrastructure | 1,200 *(from 20,000 loaded)* | 130,319 train + 11,873 val | Technical guidance on borehole maintenance, well chlorination, filtration systems |
| **Alpaca-Cleaned** | General Instructional | 500 *(from 8,000 loaded)* | 51,760 | Conversational fluency, diverse user phrasing, instruction-following structure |
| **Total Initial Corpus** | Hybrid WASH | **3,200** | 216,047 available | Balanced clinical + technical + conversational coverage |

> **ğŸ’¡ Pool Loading Strategy:** A deliberately larger candidate pool was loaded prior to filtering â€” SQuAD at 20,000 samples and Alpaca at 8,000 â€” to ensure that after applying strict WASH domain filters, the resulting dataset would reliably exceed the 1,000-sample minimum. This "load wide, filter narrow" strategy guarantees both domain purity and sufficient training volume simultaneously.

### 4.2 Why These Three Datasets?

Each dataset was chosen for a specific, non-redundant contribution to the model's capability profile:

#### ğŸ”¬ MedAlpaca Medical Flashcards â†’ Clinical Authority

MedAlpaca provides the model with clinical authority â€” the ability to speak accurately and confidently about health emergencies involving waterborne disease. The flashcard format (question â†’ precise clinical answer) is an excellent match for the structured, factual responses required when a user asks: *"What are the early signs of cholera?"* or *"How do I prepare ORS if I have no pharmacy nearby?"* Without this dataset, the model would lack the clinical depth to handle disease-related queries with the seriousness they demand.

#### ğŸ”§ SQuAD v2 (Filtered) â†’ Infrastructure Troubleshooting

SQuAD v2's question-answer structure mirrors exactly how real users interact with a technical support assistant. A community member standing in front of a broken handpump does not read a manual â€” they ask a question: *"Why is my borehole pump producing low yield?"* The filtered subset provides the model with structured, actionable answers to precisely this type of technical troubleshooting query. The rigorous multi-passage nature of SQuAD also trains the model to handle specificity and disambiguation, reducing vague or unhelpful responses.

#### ğŸ’¬ Alpaca-Cleaned â†’ Conversational Robustness

Real users do not phrase their questions in textbook English. They ask: *"Help me clean water"*, *"My kid got diarrhea"*, *"Water smells funny"*, or *"The pump broke what do I do."* The Alpaca-Cleaned dataset teaches the model to interpret and respond naturally to this range of phrasings, question structures, and levels of formality â€” ensuring that WASH guidance is accessible to users regardless of their education level or language proficiency.

---

## 5. Preprocessing Pipeline

### 5.1 Overview

Raw text data from public datasets â€” even curated ones â€” arrives with inconsistencies, noise, duplicates, and off-topic content that can degrade model performance if not systematically addressed. A rigorous **five-stage preprocessing pipeline** was applied to the full candidate pool of 3,200+ samples before any training data was finalised.

Each stage targets a distinct class of data quality problem, and the stages are applied sequentially so that each benefits from the work of the previous:

```
  Raw Corpus (~3,200+ candidates from three sources)
           â”‚
           â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  STAGE 1: Schema Validation &               â•‘
  â•‘  Type Enforcement                           â•‘
  â•‘                                             â•‘
  â•‘  â†’ Ensures instruction/response fields      â•‘
  â•‘    exist as non-empty, non-null strings     â•‘
  â•‘  â†’ Removes null, empty, whitespace-only     â•‘
  â•‘    records before any further processing    â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         â”‚
                         â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  STAGE 2: Text Normalization                â•‘
  â•‘                                             â•‘
  â•‘  â†’ Unicode NFC normalization                â•‘
  â•‘  â†’ HTML tag removal                         â•‘
  â•‘  â†’ Whitespace compression                   â•‘
  â•‘  â†’ Technical units preserved:               â•‘
  â•‘    ml, liters, %, minutes, mg/L             â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         â”‚
                         â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  STAGE 3: Exact Duplicate Removal           â•‘
  â•‘                                             â•‘
  â•‘  â†’ Removes identical (instruction,          â•‘
  â•‘    response) pairs                          â•‘
  â•‘  â†’ Prevents memorization bias from          â•‘
  â•‘    repeated training signals                â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         â”‚
                         â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  STAGE 4: Short Sample Filtering            â•‘
  â•‘                                             â•‘
  â•‘  â†’ Removes samples with < 3 words in        â•‘
  â•‘    either instruction OR response           â•‘
  â•‘  â†’ Eliminates uninformative or              â•‘
  â•‘    degenerate training examples             â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         â”‚
                         â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  STAGE 5: WASH Domain Filtering             â•‘
  â•‘                                             â•‘
  â•‘  â†’ Three-stage fallback gate:               â•‘
  â•‘    A (strict keyword) â†’                     â•‘
  â•‘    B (broad keyword) â†’                      â•‘
  â•‘    C (semantic similarity)                  â•‘
  â•‘  â†’ Guarantees â‰¥ 1,000 WASH samples          â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                         â”‚
                         â–¼
        âœ… â‰¥ 1,000 WASH-Aligned Training Samples
```

### 5.2 Stage Implementation Details

#### Stage 1 â€” Schema Validation & Type Enforcement

```python
def validate_and_cast_schema(df: pd.DataFrame) -> pd.DataFrame:
    # Ensures instruction/response exist as non-empty strings
    # Removes null, empty, and whitespace-only records
```

This stage is critical because downstream stages (tokenization, template formatting) assume non-null string fields. Failing to enforce this results in silent training errors that corrupt loss computation without raising exceptions.

#### Stage 2 â€” Text Normalization

```python
def normalize_text(text: str) -> str:
    # Unicode NFC normalization       â†’ resolves encoding inconsistencies
    # HTML tag removal                â†’ removes <p>, <br/>, &amp; artefacts
    # Whitespace compression          â†’ collapses multiple spaces/newlines
    # Preserves technical units       â†’ ml, liters, %, minutes, mg/L
```

Unicode normalization is particularly important when combining datasets from different sources (medical, QA, general), which may encode the same characters differently. Technical units such as `mg/L` and `ml` are explicitly preserved to prevent the model from learning corrupted representations of dosage and measurement language â€” which would be directly harmful in a WASH health context.

#### Stage 3 â€” Exact Duplicate Removal

Identical `(instruction, response)` pairs are removed to prevent the model from over-fitting to repeated examples, which creates memorization bias rather than genuine generalisation. This is especially important given that multiple source datasets may contain overlapping content about common WASH topics.

#### Stage 4 â€” Short Sample Filtering

Any sample where either the instruction or the response contains fewer than three words is removed. These degenerate examples contribute no meaningful signal and can introduce noise into the training loss, potentially destabilizing gradient updates in early training steps.

#### Stage 5 â€” WASH Domain Filtering (Three-Stage Gate)

The domain filtering stage is the most consequential step in the pipeline, and the one most directly responsible for the assistant's specialisation quality:

```
  Stage A â€” Strict Gate (High Precision):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Regex-based WASH keyword matching applied to full text.
  Sample retained if keyword hit count â‰¥ 1 in instruction OR response.

         â†“ if resulting count < 1,000 samples (fallback triggered)

  Stage B â€” Broad Gate (Higher Recall):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Expanded synonym and phrase patterns applied.
  Captures paraphrased WASH content missed by Stage A's strict patterns.
  Guarantees â‰¥ 1,000 samples post-filter.

         â†“ if still below minimum (edge case only)

  Stage C â€” Semantic Top-Up (Optional, Embedding-Based):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Sentence embedding similarity computed against a WASH anchor description.
  Threshold: cosine_similarity â‰¥ 0.35
  Enabled only if count remains below minimum after Stages A and B.
```

> **Note on Experiment 5:** The strict-filter experiment (â‰¥2 keyword hits) was run as a controlled variant of Stage A to measure the impact of domain purity on model performance. Results showed improved semantic alignment (BERTScore-F1) at the cost of slight lexical diversity reduction â€” confirming that filtering threshold is a meaningful hyperparameter in its own right, not just a data engineering detail.

### 5.3 WASH Keyword Vocabulary (Representative Subset)

```python
# Water Treatment & Safety
water, drinking water, chlorine, bleach, disinfect, purify, filter,
contamination, turbidity, pathogen, boil, sedimentation, fluoride

# Sanitation & Hygiene
sanitation, hygiene, handwashing, latrine, toilet, sewage,
wastewater, faeces, open defecation, menstrual hygiene

# Water Infrastructure
borehole, handpump, well, pipe, pump, maintenance, repair,
leak, valve, storage tank, submersible, yield, aquifer

# Public Health & Clinical
cholera, typhoid, diarrhea, diarrhoea, dehydration, ORS,
oral rehydration, infection, public health, dysentery,
waterborne, gastroenteritis, stunting
```

### 5.4 Preprocessing Audit Summary

| Stage | Records Remaining | Action Taken |
|-------|:-----------------:|--------------|
| Original Candidate Pool | ~3,200+ | Initial corpus loaded from 3 sources |
| After Schema Validation | Reduced | Nulls, empties, and whitespace-only rows removed |
| After Normalization | Same count | Non-destructive text cleaning applied |
| After Duplicate Removal | Reduced | Exact `(instruction, response)` pairs deduplicated |
| After Short-Sample Filter | Reduced | Records with < 3 words in either field removed |
| **After Domain Filtering** | **â‰¥ 1,000 âœ…** | **WASH-aligned samples guaranteed** |

---

## 6. Model Architecture

### 6.1 Overview

The choice of base model is one of the most consequential decisions in any fine-tuning project. The selected model must be capable enough to produce coherent, multi-sentence domain responses, yet compact enough to train and serve within the compute budget of a free-tier cloud GPU. After systematic evaluation of available options, **TinyLlama-1.1B-Chat-v1.0** was selected as the base model for this project.

### 6.2 Base Model: TinyLlama-1.1B-Chat-v1.0

| Property | Value |
|----------|-------|
| **Architecture** | Causal Decoder-Only Transformer |
| **Total Parameters** | 1.1 Billion |
| **Tokenizer** | SentencePiece (BPE-style subword encoding) |
| **Pre-training Objective** | Chat/dialogue optimized (instruction-following alignment) |
| **Context Window** | Up to 2,048 tokens (project uses 512) |
| **Hugging Face ID** | `TinyLlama/TinyLlama-1.1B-Chat-v1.0` |

TinyLlama's chat alignment â€” achieved through instruction-following pre-training â€” significantly reduces the fine-tuning burden for this project. The base model already understands the `User: ... / Assistant: ...` conversational format, which means domain adaptation (not conversational format learning) is the primary fine-tuning objective. Every gradient step spent in fine-tuning goes toward learning WASH-specific knowledge rather than learning how to answer questions.

### 6.3 Model Selection Rationale â€” Why Not Other Models?

A systematic comparison was conducted before selecting TinyLlama:

| Model | Parameters | Architecture | Key Limitation | Rejection Reason |
|-------|:----------:|:------------:|----------------|-----------------|
| **BERT-Base** | 110M | Encoder-only | Classification only, cannot generate free text | Cannot produce dynamic multi-sentence conversational responses â€” fundamentally incompatible with the generative QA objective |
| **T5-Small** | 60M | Encoder-Decoder | Dual-pass processing, higher architectural complexity | Less efficient for open-ended causal generation; T5's text-to-text format adds unnecessary overhead for conversational use |
| **GPT-2** | 124M | Decoder-only | Not chat-optimised, older and less capable architecture | No conversational alignment; would require extensive format learning before domain adaptation could begin, wasting the training budget |
| **âœ… TinyLlama-1.1B** | **1.1B** | **Decoder-only** | None for project constraints | Chat pre-training, 4-bit quantization fits T4 VRAM, coherent multi-sentence technical outputs, aligns perfectly with generative QA objectives |

The decisive factors for selecting TinyLlama were:

- **Chat pre-training** directly reduces fine-tuning burden â€” the model already knows how to follow instructions and produce structured answers
- **4-bit quantization** compresses the 1.1B model to ~1.1 GB, fitting comfortably within the T4's 15 GB VRAM with room for adapter training
- **Decoder-only architecture** is ideal for autoregressive generative QA â€” each token is predicted sequentially given all previous context, producing coherent, contextually grounded answers
- **Proven conversational coherence** â€” TinyLlama produces multi-sentence technical instructions with appropriate structure, flow, and domain-appropriate vocabulary even before fine-tuning

---

## 7. Fine-Tuning Strategy â€” QLoRA + LoRA

### 7.1 Why Not Full Fine-Tuning?

Full fine-tuning of a 1.1B parameter model would require updating every weight in the network across every training step. On a T4 GPU with 15 GB VRAM, this is computationally infeasible â€” the base model alone consumes approximately 4.4 GB in full precision (float32), and the gradient storage, optimizer states, and activation checkpoints required during training would push total VRAM requirements to 40â€“80 GB or more. Even with paid compute, full fine-tuning of a 1.1B model for domain adaptation is wasteful: most of the model's general linguistic knowledge should be preserved, not overwritten.

**QLoRA + LoRA** solves this problem elegantly and efficiently:

```
  Full Fine-Tuning:  ALL ~1.1B params updated  â†’  ~40â€“80 GB VRAM required
                     Every weight gradient stored  â†’  Hours of training

  QLoRA + LoRA:      Base model frozen in 4-bit  â†’  ~1.1 GB
                     Only LoRA adapter params updated  â†’  minimal
                     Total peak VRAM: ~2.28 GB  âœ…
                     Training time: ~24.8 min  âœ…
```

### 7.2 QLoRA â€” Quantized LoRA (Dettmers et al., 2023)

QLoRA achieves parameter-efficient fine-tuning through two quantization mechanisms applied in sequence:

**4-bit NF4 Quantization:**
The base model weights are quantized from 32-bit float to 4-bit NormalFloat (NF4) format before training begins. This reduces the base model's memory footprint from ~4.4 GB to ~1.1 GB â€” a 75% reduction â€” while preserving the weight distribution's statistical properties through a normally-distributed quantization grid optimally suited for neural network weight distributions.

**Double Quantization:**
The quantization constants themselves are further quantized, recovering an additional ~0.4 bits per parameter and further reducing memory footprint without impacting final model quality.

**During training:** The base model remains completely frozen in 4-bit NF4. Computations are cast to float16 for numerical stability. Only the small LoRA adapter matrices are updated in full precision â€” ensuring training stability without VRAM overhead.

### 7.3 LoRA â€” Low-Rank Adaptation (Hu et al., 2021)

LoRA addresses the fundamental question of parameter-efficient adaptation: *if we must add trainable parameters to a frozen model, where should we add them and what form should they take?*

The answer exploits a key insight from neural network theory: **the weight updates required to adapt a pre-trained model to a new task tend to be low-rank** â€” meaning they can be well-approximated by the product of two much smaller matrices, with minimal loss of adaptation quality.

**Mathematical Formulation:**

```
  Standard weight update:
    W_new = W_frozen + Î”W

  LoRA decomposes Î”W as:
    Î”W = A Ã— B

  Where:
    A  âˆˆ  â„^(d Ã— r)   "down-projection" matrix  (random initialization)
    B  âˆˆ  â„^(r Ã— k)   "up-projection"  matrix   (zero initialization)
    r  =  rank         controls adaptation capacity  (r << d, r << k)

  The full adapted weight is:
    W_new = W_frozen + (A Ã— B) Ã— (Î± / r)

  Where Î± = lora_alpha is a scaling hyperparameter that controls
  the magnitude of the adaptation relative to the frozen weights.

  Key property: Only A and B are trained
  â†’ Drastically fewer parameters than full Î”W
  â†’ WASH domain adaptation with minimal compute
```

**Why target `q_proj` and `v_proj`?**

These are the Query and Value projection matrices in the self-attention mechanism. Research (Hu et al., 2021) and empirical results consistently show that adapting these two matrices captures the majority of task-relevant adaptation. The Query projection determines what information each token attends to; the Value projection determines what content flows forward once attention weights are computed. Adapting these two matrices is sufficient for domain specialisation without injecting adapters into every layer.

### 7.4 Training Configuration

```python
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 1: Quantization Configuration (QLoRA)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from transformers import BitsAndBytesConfig
import torch

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",              # NormalFloat 4-bit quantization
    bnb_4bit_compute_dtype=torch.float16,   # Cast computations to float16
    bnb_4bit_use_double_quant=True          # Quantize the quantization constants
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 2: LoRA Adapter Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from peft import LoraConfig

lora_config = LoraConfig(
    r=16,                         # Rank â€” controls adaptation capacity
    lora_alpha=32,                # Scaling factor (Î±); effective scale = Î±/r = 2
    lora_dropout=0.05,            # Dropout for regularization
    bias="none",                  # No bias terms added
    task_type="CAUSAL_LM",        # Causal language modeling task
    target_modules=["q_proj", "v_proj"]   # Attention projection layers only
)
```

### 7.5 Chat Template & Input Formatting

All training samples were formatted as a unified conversational template consistent with TinyLlama's chat pre-training format:

**Training template (instruction-following format):**
```
User: <instruction>
Assistant: <response>
```

**Gradio UI inference template (Alpaca-style, for user clarity):**
```
### Instruction:
<user_question>

### Response:
<assistant_answer>
```

### 7.6 Tokenization Strategy & Justification

```python
tokenizer(
    text,
    max_length=512,        # Covers ~99% of samples without truncation
    truncation=True,       # Handles the ~1% edge cases gracefully
    padding="max_length"   # Right-padding for causal decoder training stability
)
```

The `max_length=512` value was validated empirically by computing the cumulative distribution function (CDF) of token lengths across the full preprocessed dataset before training. CDF analysis confirmed that 512 tokens covers approximately 99% of all samples â€” making it the optimal balance between complete coverage and VRAM efficiency. A context window shorter than 512 would truncate clinically meaningful content in longer disease management responses; longer would waste VRAM on predominantly empty padding positions.

---

## 8. Experimental Framework

### 8.1 Design Philosophy

The experimental framework follows the scientific principle of **controlled single-variable comparison**: five experiments, each differing from the primary baseline (Experiment 1) by exactly one hyperparameter. This design isolates the individual contribution of each variable to final model quality and enables clean, scientifically grounded conclusions. Results are not confounded by simultaneous changes to multiple parameters â€” a common weakness in less rigorous fine-tuning pipelines.

A zero-shot baseline (Experiment 0) was also evaluated to quantify the improvement attributable to fine-tuning itself â€” establishing the minimum performance floor and demonstrating that domain-specific training provides measurable, reproducible gains.

### 8.2 Experiment Overview

| Experiment | Key Change | Learning Rate | Steps | LoRA `r` | Domain Filter | Purpose |
|------------|------------|:-------------:|:-----:|:---------:|:-------------:|---------|
| **Baseline (Exp 0)** | Zero-shot pre-trained TinyLlama | â€” | â€” | â€” | â€” | Establish pre-fine-tuning reference floor |
| **Exp 1** | Standard QLoRA + LoRA | `2e-4` | 300 | 16 | â‰¥ 1 hit | Establish fine-tuned performance baseline |
| **Exp 2** | Lower learning rate only | `5e-5` | 300 | 16 | â‰¥ 1 hit | Measure LR sensitivity and convergence speed |
| **Exp 3** | Reduced LoRA rank only | `2e-4` | 300 | 8 | â‰¥ 1 hit | Test parameter efficiency trade-off |
| **Exp 4** | Fewer training steps only | `2e-4` | 200 | 16 | â‰¥ 1 hit | Identify compute-efficiency knee point |
| **Exp 5** | Stricter domain filter only | `2e-4` | 300 | 16 | â‰¥ 2 hits | Quantify preprocessing quality's impact on quality |

### 8.3 Shared Hyperparameters (All Experiments)

These parameters were held constant across all five experiments. Any observed performance differences between experiments are attributable solely to the single variable being manipulated:

| Parameter | Value | Justification |
|-----------|:-----:|---------------|
| Quantization | 4-bit NF4 | T4 GPU memory compatibility constraint |
| Batch Size (per device) | 2 | Maximum stable batch on T4 without OOM error |
| Gradient Accumulation | 4 steps | Effective batch size of 8; stable gradient signal |
| Warmup Ratio | 0.03 | Prevents destructive gradient updates in early training steps |
| Weight Decay | 0.01 | L2 regularization to reduce overfitting on small dataset |
| fp16 | `False` | Avoids T4-specific mixed-precision instability |
| bf16 | `False` | Not supported on T4 GPU architecture |
| `eval_strategy` | `"steps"` | Step-based validation (corrected Transformers API parameter name) |
| `eval_steps` | 50 | Frequent checkpointing for granular loss monitoring |
| `logging_steps` | 25 | High-resolution training loss tracking |

### 8.4 Experiment 1 â€” Detailed Training Progression
**Configuration:** `r=16, LR=2e-4, 300 steps, domain filter â‰¥1 keyword hit`

> â±ï¸ **Training time:** 1,486.3 seconds (~24.8 min) &nbsp;&nbsp;|&nbsp;&nbsp; ğŸ–¥ï¸ **Peak GPU memory:** 2.28 GB

| Step | Training Loss | Validation Loss | Interpretation |
|:----:|:------------:|:---------------:|----------------|
| 50 | 1.1847 | 1.1466 | Initial rapid adaptation to WASH domain |
| 100 | 1.1084 | 1.0936 | Strong continued improvement â€” model learning fast |
| 150 | 1.0508 | 1.0857 | Approaching validation plateau |
| 200 | 1.0478 | 1.0841 | Near-plateau â€” majority of learning complete |
| 250 | 1.0832 | 1.0818 | Stable â€” training loss slight uptick (expected) |
| **300** | **1.1140** | **1.0814** | **Converged âœ… â€” validation at minimum** |

```
  Experiment 1 â€” Loss Curves (Training vs Validation)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Loss
  1.22 â”‚
  1.20 â”‚ â— â† Training Loss starts high, descends rapidly
  1.18 â”‚   â•²
  1.15 â”‚    â—
  1.12 â”‚     â•²
  1.10 â”‚      â—â”€â”€â”€â—   â— â† Training Loss slight uptick at 250â€“300
  1.08 â”‚            â—   â— â† Val Loss plateaus ~1.081 (minimum)
  1.06 â”‚
  1.05 â”‚          â—â”€â”€â”€â—   â† Training Loss minimum at steps 150â€“200
  1.02 â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              50     100     150     200     250     300   Step

  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Final Validation Loss : 1.0814
  Final Perplexity      : exp(1.0814) â‰ˆ 2.95
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### 8.5 Experiment 2 â€” Training Progression (Lower LR = 5e-5)
**Configuration:** `r=16, LR=5e-5, 300 steps` â€” isolates learning rate effect

| Step | Training Loss | Validation Loss | Interpretation |
|:----:|:------------:|:---------------:|----------------|
| 50 | 1.4653 | 1.4137 | Very slow start â€” LR too cautious for 300-step budget |
| 100 | 1.2092 | 1.1934 | Gradual but delayed descent |
| 150 | 1.1265 | 1.1679 | Still well above Exp 1 at same step |
| 200 | 1.1262 | 1.1584 | Slower convergence than Exp 1 at step 150 |
| 250 | 1.1598 | 1.1548 | Slight oscillation; not yet stable |
| **300** | **1.1982** | **1.1544** | **Not fully converged â€” budget exhausted** |

```
  Final Validation Loss : 1.1544
  Final Perplexity      : exp(1.1544) â‰ˆ 3.17
  Î” vs Experiment 1     : +7.5% worse â€” confirms LR=2e-4 is optimal
```

### 8.6 Experiment 4 â€” Training Progression (Shorter Schedule: 200 Steps)
**Configuration:** `r=16, LR=2e-4, 200 steps` â€” isolates training duration effect

| Step | Training Loss | Validation Loss | Interpretation |
|:----:|:------------:|:---------------:|----------------|
| 50 | 1.1830 | 1.1483 | Strong initial descent â€” identical to Exp 1 start |
| 100 | 1.1102 | 1.0959 | Rapid improvement â€” model adapting fast |
| 150 | 1.0542 | 1.0885 | Near-convergence already reached |
| **200** | **1.0533** | **1.0871** | **Converged âœ… â€” nearly identical to Exp 1** |

```
  Final Validation Loss : 1.0871
  Final Perplexity      : exp(1.0871) â‰ˆ 2.97
  Î” vs Experiment 1     : only +0.7% worse â€” with 33% less compute

  Compute Efficiency:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  200 steps  â†’  ~16.5 min  â†’  Perplexity â‰ˆ 2.97  (67% of compute)
  300 steps  â†’  ~24.8 min  â†’  Perplexity â‰ˆ 2.95  (100% of compute)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  33% more compute â†’ only 0.7% performance gain
  â†’ Strong evidence of diminishing returns beyond 200 steps
```

---

## 9. Evaluation Metrics

### 9.1 Overview

Evaluating a generative language model for domain-specific quality requires a multi-dimensional metric framework. No single metric is sufficient because each captures a different aspect of response quality that matters in real-world WASH deployment. Six complementary metrics were computed consistently across all experiments, spanning lexical overlap, semantic similarity, language fluency, and safety:

| Metric | Type | What It Measures | Relevance to WASH |
|--------|:----:|-----------------|------------------|
| **BLEU** | Lexical | N-gram precision overlap between model output and reference | Measures surface-level accuracy; good for evaluating factual precision in technical responses |
| **ROUGE-L** | Lexical | Longest Common Subsequence (LCS) similarity between output and reference | Captures whether key content sequences appear in output, even with paraphrasing |
| **BERTScore-F1** | Semantic | Contextual embedding similarity using BERT representations | Captures meaning-level quality; robust to synonym use and clinical paraphrasing |
| **Token-Level F1** | Lexical | Word-overlap precision/recall harmonic mean | Lightweight overlap metric; complements BLEU without n-gram order dependency |
| **Perplexity** | Fluency | `exp(eval_loss)` â€” model confidence in its own generated tokens | Lower perplexity = more fluent, predictable, well-calibrated language model |
| **OOD Refusal Rate** | Safety | % of out-of-domain queries correctly refused rather than answered | Most operationally critical metric â€” directly measures safety in health-sensitive deployment |

### 9.2 Token-Level F1 Implementation

```python
def token_f1_overlap(pred: str, ref: str) -> float:
    """
    Computes token-level F1 between prediction and reference.
    Uses set intersection to measure shared vocabulary coverage.
    Robust to word ordering differences unlike BLEU.
    """
    pred_set = set(pred.lower().split())
    ref_set  = set(ref.lower().split())

    # True Positives: tokens present in both prediction and reference
    tp        = len(pred_set.intersection(ref_set))

    precision = tp / max(len(pred_set), 1)   # How much of pred is relevant?
    recall    = tp / max(len(ref_set),  1)   # How much of ref was captured?

    if precision + recall == 0:
        return 0.0

    # Harmonic mean of precision and recall
    return 2 * (precision * recall) / (precision + recall)
```

### 9.3 Why BERTScore Is the Most Clinically Meaningful Metric

In the WASH domain, correct meaning matters more than exact wording. A response that states *"add 2 drops of chlorine solution per litre of water"* is functionally equivalent to *"use 2 drops of sodium hypochlorite per liter"*. BLEU would penalise this paraphrase as an incorrect answer; BERTScore-F1 would correctly recognise the semantic equivalence through contextual embedding comparison.

For health-critical guidance where the goal is accurate meaning delivery rather than verbatim reproduction, BERTScore-F1 is therefore the most clinically meaningful evaluation metric. Experiments are compared primarily on perplexity (fluency) and BERTScore-F1 (semantic accuracy), with BLEU and ROUGE-L providing supporting lexical evidence.

---

## 10. Results & Analysis

### 10.1 Cross-Experiment Comparison

| Experiment | Eval Loss | Perplexity | BLEU | ROUGE-L | BERTScore-F1 | Token-F1 | OOD Refusal |
|------------|:---------:|:----------:|:----:|:-------:|:------------:|:--------:|:-----------:|
| **Baseline (Exp 0)** | â€” | Higher | Lower | Lower | Lower | Lower | Programmatic |
| **Exp 1** *(r=16, LR=2e-4, 300 steps)* | 1.0814 | â‰ˆ **2.95** | Logged âœ… | Logged âœ… | Logged âœ… | Logged âœ… | **100%** |
| **Exp 2** *(LR=5e-5, 300 steps)* | 1.1544 | â‰ˆ 3.17 | Logged âœ… | Logged âœ… | Logged âœ… | Logged âœ… | **100%** |
| **Exp 3** *(r=8, all else same as Exp 1)* | Logged | Logged | Logged âœ… | Logged âœ… | Logged âœ… | Logged âœ… | **100%** |
| **Exp 4** *(200 steps, all else same as Exp 1)* | 1.0871 | â‰ˆ 2.97 | Logged âœ… | Logged âœ… | Logged âœ… | Logged âœ… | **100%** |
| **Exp 5** *(strict filter â‰¥2 hits)* | Logged | Logged | Logged âœ… | Logged âœ… | Logged âœ… | Logged âœ… | **100%** |

> âœ… All fine-tuned experiments showed improvement over the zero-shot baseline across multiple metrics.
> ğŸ“Š Exact BLEU/ROUGE/BERTScore numerical values are computed and stored in the notebook's `experiment_results` DataFrame for full reproducibility.

### 10.2 Perplexity Comparison

```
  Perplexity by Experiment  (lower = better fluency & confidence)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  3.20 â”‚              â–ˆâ–ˆ
  3.17 â”‚              â–ˆâ–ˆ  â† Exp 2: LR too low, underfitting
  3.15 â”‚              â–ˆâ–ˆ
  3.10 â”‚              â–ˆâ–ˆ
  3.05 â”‚              â–ˆâ–ˆ
  3.00 â”‚              â–ˆâ–ˆ
  2.97 â”‚  â–ˆâ–ˆ           â–ˆâ–ˆ          â–ˆâ–ˆ
  2.95 â”‚  â–ˆâ–ˆ    â–ˆâ–ˆ     â–ˆâ–ˆ    â–ˆâ–ˆ    â–ˆâ–ˆ
  2.93 â”‚  â–ˆâ–ˆ    â–ˆâ–ˆ     â–ˆâ–ˆ    â–ˆâ–ˆ    â–ˆâ–ˆ
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          Exp1   Exp3   Exp2  Exp4   Exp5
         (best) (r=8)  (LRâ†“) (200s) (strict)

  Note: Exp1 = optimal reference | Exp2 = worst within fine-tuned
        Exp4 = near-best with 33% less compute (most efficient)
```

### 10.3 Key Findings â€” Four Experimental Dimensions

#### ğŸ”¬ Finding 1: Learning Rate Is Critical (Exp 1 vs Exp 2)

Within a fixed 300-step budget, `LR = 2e-4` substantially outperformed `LR = 5e-5`. The lower learning rate produced sluggish convergence â€” the validation loss at step 300 for Experiment 2 (1.1544) had still not reached the performance that Experiment 1 achieved at step 200 (1.0841). This confirms that learning rate is a more impactful hyperparameter than step count when operating under tight training budgets. The lower LR causes underfitting, not stability.

```
  LR = 2e-4  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Perplexity â‰ˆ 2.95  âœ… OPTIMAL
  LR = 5e-5  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Perplexity â‰ˆ 3.17  âŒ +7.5% worse
```

**Conclusion:** `LR = 2e-4` is optimal for this dataset size and training budget. Practitioners adapting this framework should not reduce LR below `1e-4` without proportionally increasing step count.

#### ğŸ”¬ Finding 2: Reduced LoRA Rank Is Viable (Exp 1 vs Exp 3)

Reducing LoRA rank from 16 to 8 halved the number of trainable adapter parameters while producing only marginal performance decrease across most metrics. This is a critical finding for resource-constrained deployment: the WASH specialisation task does not require large adapter matrices. The semantic differences between r=16 and r=8 in this domain are small enough to be operationally irrelevant in most field deployment scenarios.

```
  r = 16  â”€â”€â”€â”€â”€  Higher capacity adapters  â”€â”€â”€  Standard performance  (baseline)
  r = 8   â”€â”€â”€â”€â”€  Reduced GPU memory        â”€â”€â”€  Minimal performance drop
```

**Conclusion:** `r=8` is a fully viable configuration for mobile or edge deployments where even marginal VRAM savings matter. WASH domain specialisation does not require high-rank adapters.

#### ğŸ”¬ Finding 3: Strong Diminishing Returns Beyond 200 Steps (Exp 1 vs Exp 4)

Domain adaptation with QLoRA + LoRA exhibits rapid convergence. The vast majority of meaningful learning occurs within the first 150 steps, and performance improvements from steps 200â€“300 are marginal at 0.7%. This has significant practical implications â€” it means that rapid domain adaptation for new WASH guidelines or regional updates could be achieved in under 20 minutes on free Colab compute without meaningful quality degradation.

```
  300 steps  â†’  ~24.8 min  â†’  Perplexity â‰ˆ 2.95   (100% of compute)
  200 steps  â†’  ~16.5 min  â†’  Perplexity â‰ˆ 2.97   (67% of compute)

  Efficiency: 33% less compute achieves 99.3% of full-schedule performance
```

**Conclusion:** 200 steps is the optimal training duration for this setup. For rapid iteration or data updates, 150 steps may suffice for initial validation.

#### ğŸ”¬ Finding 4: Domain Filtering Quality Directly Impacts Semantic Quality (Exp 1 vs Exp 5)

Increasing the keyword threshold from â‰¥1 to â‰¥2 hits produced a measurable improvement in BERTScore-F1 (semantic alignment) at the cost of slightly reduced lexical diversity. This confirms an important principle: **training data quality is a measurable hyperparameter**, not simply a preprocessing best-practice. A smaller, higher-purity dataset can outperform a larger, noisier one for domain-specific tasks.

```
  â‰¥ 1 keyword hit  â†’  Broader dataset coverage  â†’  Higher lexical diversity
  â‰¥ 2 keyword hits â†’  Stricter domain purity     â†’  Improved BERTScore-F1
```

**Conclusion:** For production deployment, â‰¥2 keyword hits is recommended when dataset size permits. Domain filtering strictness should be treated as a hyperparameter to tune alongside learning rate and rank.

### 10.4 Best Model Selection â€” Composite Scoring

The best experiment is selected programmatically using a normalized composite multi-metric score, ensuring that no single metric disproportionately influences the selection:

```python
composite_score = mean([
    normalize(BLEU),           # Lexical precision component
    normalize(ROUGE_L),        # Content coverage component
    normalize(BERTScore_F1),   # Semantic alignment component (highest weight via mean)
    normalize(Token_F1),       # Word-overlap balance component
    1 - normalize(Perplexity)  # Fluency component (inverted â€” lower perplexity = better)
])

# Hard safety constraint â€” cannot select a model that fails domain enforcement
assert OOD_Refusal_Rate >= baseline_programmatic_refusal_rate
```

---

## 11. Domain Boundary Handling

### 11.1 Why Domain Boundary Enforcement Is Non-Negotiable

In most consumer AI applications, hallucination â€” the model generating plausible-sounding but factually incorrect information â€” is an inconvenience that erodes user trust. In a WASH health context, hallucination is not an inconvenience. **It can be lethal.**

A model that confidently provides incorrect chlorination dosing, incorrect ORS salt ratios, or incorrect guidance on cholera rehydration is not a neutral, imperfect tool â€” it is an actively dangerous one. The stakes are highest for the most vulnerable users: community health workers without medical training, caregivers of sick children, field technicians diagnosing infrastructure failures in remote areas.

The Temba Digital Bridge addresses this risk through **two-layer domain boundary enforcement**: a fast keyword gate that filters obvious out-of-domain queries, backed by an optional semantic similarity gate that handles more subtle cases. When either gate determines that a query is outside the WASH domain, the model does not attempt to generate an answer. It returns a fixed, professional refusal message.

### 11.2 The Official Refusal Message

When a query is detected as out-of-domain, the system returns exactly the following message, stored as the `OUT_OF_DOMAIN_RESPONSE` constant and used identically across all experiments and the Gradio UI:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                              â”‚
â”‚  "I'm specialized in water, sanitation, infrastructure, and public health   â”‚
â”‚   topics. This question seems to be outside my area of expertise. Please    â”‚
â”‚   contact our team for assistance with other topics. If your concern         â”‚
â”‚   relates to water safety, sanitation, hygiene, or infrastructure, kindly   â”‚
â”‚   rephrase your question and I'll gladly assist you."                       â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The message is professional, non-dismissive, and constructive â€” it directs the user toward the correct resource while inviting them to rephrase if their concern is genuinely WASH-related but poorly worded.

### 11.3 Two-Layer Domain Gate Architecture

```
  User Query Submitted
         â”‚
         â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  LAYER 1 â€” KEYWORD GATE                                             â•‘
  â•‘  (Primary | Fast | Deterministic | Zero Latency)                    â•‘
  â•‘                                                                      â•‘
  â•‘  Regex-based pattern matching across full WASH keyword vocabulary    â•‘
  â•‘                                                                      â•‘
  â•‘  domain_score = count_of_WASH_regex_matches(query)                  â•‘
  â•‘                                                                      â•‘
  â•‘  if domain_score < KEYWORD_DOMAIN_THRESHOLD (default = 1):          â•‘
  â•‘      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
  â•‘      return OUT_OF_DOMAIN_RESPONSE     [END â€” no generation]        â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                             â”‚ PASS (â‰¥1 WASH keyword found)
                                             â–¼
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  LAYER 2 â€” SEMANTIC GATE                                            â•‘
  â•‘  (Optional | Robust | Embedding-Based | Catches Subtle OOD)         â•‘
  â•‘                                                                      â•‘
  â•‘  Computes cosine similarity between query embedding and a           â•‘
  â•‘  pre-defined WASH anchor description embedding                       â•‘
  â•‘                                                                      â•‘
  â•‘  similarity = cosine_similarity(                                     â•‘
  â•‘      embed(query),                                                   â•‘
  â•‘      embed(WASH_anchor_description)                                  â•‘
  â•‘  )                                                                   â•‘
  â•‘                                                                      â•‘
  â•‘  if similarity < SEMANTIC_THRESHOLD (0.35):                         â•‘
  â•‘      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘
  â•‘      return OUT_OF_DOMAIN_RESPONSE     [END â€” no generation]        â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                             â”‚ PASS (semantically in-domain)
                                             â–¼
                              âœ… Proceed to Model Generation
```

Layer 1 is fast and rule-based â€” it catches clear out-of-domain queries with zero computational overhead. Layer 2 is deeper and embedding-based â€” it catches queries that contain WASH-adjacent vocabulary but are not genuinely asking about water or sanitation topics (e.g., *"What ocean percentage is fresh water?"* â€” contains "water" but is a geography question, not a WASH query). Together they provide robust protection without introducing meaningful inference latency for valid WASH queries.

### 11.4 OOD Stress Test Prompts

**âœ… In-Domain Prompts (Expected: Model generates a response)**

| # | Prompt | WASH Domain |
|---|--------|:-----------:|
| 1 | "How can I disinfect drinking water at home if I have no filter?" | ğŸ’§ Water Safety |
| 2 | "What are the first steps to prevent cholera spread in a community?" | ğŸ¦  Public Health |
| 3 | "My borehole pump produces less water than usual. What should I check?" | ğŸ”§ Infrastructure |
| 4 | "How do I prepare oral rehydration solution (ORS) safely?" | ğŸ¦  Public Health |
| 5 | "What hygiene practices reduce diarrhea transmission in households?" | ğŸ§¼ Sanitation |

**âŒ Out-of-Domain Prompts (Expected: Return refusal message)**

| # | Prompt | OOD Category |
|---|--------|:------------:|
| 1 | "Who won the last Champions League?" | âš½ Sports |
| 2 | "Can you write me a JavaScript function for sorting an array?" | ğŸ’» Software Engineering |
| 3 | "What is the best strategy to invest in cryptocurrency?" | ğŸ’¹ Finance |
| 4 | "Explain quantum computing in simple terms." | âš›ï¸ Physics/Computing |
| 5 | "Write a poem about the ocean." | ğŸ“ Creative Writing |

### 11.5 OOD Classification â€” Confusion Matrix

|  | **Predicted: In-Domain** | **Predicted: Out-of-Domain** |
|--|:------------------------:|:----------------------------:|
| **True: In-Domain** | TP âœ… *(generate correctly)* | FN âŒ *(incorrectly refused â€” lost utility)* |
| **True: Out-of-Domain** | FP âš ï¸ *(dangerous hallucination â€” most critical failure)* | TN âœ… *(correctly refused â€” safe)* |

> **Most Critical Failure Mode:** A False Positive (FP) means the model generated a confident-sounding response to an out-of-domain query instead of refusing. In health-sensitive WASH deployment, this is the failure mode that could directly cause harm. The two-layer gate architecture is specifically designed to drive FP rate to zero on all tested prompts.
>
> **Result: All tested experiments achieved 100% OOD Refusal Rate on the full stress test prompt set.**

---

## 12. User Interface (Gradio)

### 12.1 Overview

A production-ready Gradio user interface was developed to demonstrate the assistant in a realistic deployment context. The interface was not designed as a prototype â€” it is a polished, deployment-grade application with aesthetic coherence, transparent metadata display, and robust domain enforcement integrated at the UI layer in addition to the model layer. The design reflects the water mission of the project through intentional visual choices.

### 12.2 Complete Feature Set

| Feature | Description |
|---------|-------------|
| ğŸ¨ **Custom CSS Styling** | Blue/teal water aesthetic using Plus Jakarta Sans typography; visual design reflects the water and sanitation mission |
| ğŸ’¬ **Chat Interface** | Bubble-style conversation display with copy-to-clipboard button for easy sharing of guidance with colleagues or community members |
| ğŸ“‹ **Categorized Example Prompts** | Three categories of clickable WASH example questions allow new users to immediately explore assistant capabilities without needing to compose queries |
| âš™ï¸ **Generation Settings Panel** | User-adjustable temperature, max tokens, and repetition penalty â€” enabling fine-grained control over response determinism and length |
| ğŸ“Š **Response Metadata Display** | Each assistant response includes inference time, token count, and timestamp â€” supporting transparent AI use in health contexts |
| ğŸ›¡ï¸ **Domain Boundary Enforcement** | OOD gate applied at UI layer before generation is triggered â€” the interface never sends an out-of-domain query to the model |
| ğŸ¤– **Auto Model Selection** | Automatically selects the best available trained model from the experiment variable namespace; gracefully degrades to lower experiments if full training was not completed |
| â„¹ï¸ **Model Info Card** | Displays active model name, architecture, and operational status â€” ensuring users know which model version is serving responses |

### 12.3 Auto Model Selection Priority

The UI implements a priority-ordered candidate list to gracefully handle partial training runs:

```python
_MODEL_CANDIDATES = [
    "optimized_model",   # 1st: Explicitly designated best (if post-training merge)
    "best_model",        # 2nd: Dashboard composite-score winner
    "exp1_model",        # 3rd: Experiment 1 â€” standard baseline (most robust)
    "exp4_model",        # 4th: Experiment 4 â€” efficiency-optimised
    "exp3_model",        # 5th: Experiment 3 â€” rank-reduced variant
    "exp2_model",        # 6th: Experiment 2 â€” lower LR variant
    "baseline_model",    # 7th: Zero-shot fallback (Experiment 0)
]
```

### 12.4 Example Question Categories

| Category | Example Questions Provided |
|----------|---------------------------|
| ğŸ’§ **Water Safety** | Treating drinking water at home without filter, chlorinating a community well, identifying water contamination signs, assessing turbidity |
| ğŸ§¼ **Hygiene & Sanitation** | When and how to wash hands effectively, best community sanitation practices, household waste disposal guidance |
| ğŸ¦  **Waterborne Diseases** | Recognising cholera symptoms and emergency response, preventing typhoid through water treatment, managing acute diarrhoeal disease at home, preparing ORS |

### 12.5 Recommended Generation Settings

| Setting | Default | Range | Recommendation |
|---------|:-------:|:-----:|----------------|
| **Temperature** | 0.2 | 0.1 â€“ 1.0 | 0.1â€“0.3 for factual WASH guidance; higher values increase variety but reduce reliability â€” not recommended for health-critical responses |
| **Max Tokens** | 256 | 64 â€“ 512 | 256 provides complete, detailed multi-step explanations; reduce to 128 for concise responses; 512 for comprehensive infrastructure guides |
| **Repetition Penalty** | 1.1 | 1.0 â€“ 1.5 | 1.1 effectively reduces repetitive loops in technical instructions; values above 1.3 may cause abrupt mid-sentence truncation |

---

## 13. Notebook Structure

The notebook follows a strict sequential execution design â€” each cell builds upon variables, models, and datasets produced by previous cells. The structure mirrors the complete ML pipeline from raw data ingestion through five experiments, evaluation, and final UI deployment.

| Cell | Purpose | Key Outputs |
|------|---------|-------------|
| **Cell 1** | Environment setup, global configuration, reproducibility seeding (`GLOBAL_SEED=42`) | Seeded runtime, GPU detection, dependency version snapshot, global constant definitions |
| **Cell 2** | Large candidate pool loading (3,200+ samples) and initial Exploratory Data Analysis | KDE plots of sample lengths, word cloud, source distribution pie chart, sample quality statistics |
| **Cell 3** | Five-stage preprocessing pipeline with audit logging | Cleaned dataset, preprocessing audit table, before/after KDE comparison plots |
| **Cell 4** | WASH domain filtering (keyword gate + optional semantic gate) | â‰¥1,000 domain-aligned samples, keyword frequency bar charts, filtering stage report |
| **Cell 5** | Chat template formatting and structural validation | `formatted_text`, `prompt_text`, `answer_text` columns; format verification outputs |
| **Cell 6** | Tokenization and context window analysis | Tokenized HuggingFace `Dataset`, CDF of token lengths, `max_length=512` justification plot |
| **Cell 7** | 85/15 train/validation split and baseline model loading | `dataset_splits`, `baseline_model`, baseline generation utility functions |
| **Cell 8** | Domain boundary handling implementation and OOD stress testing | `OUT_OF_DOMAIN_RESPONSE` constant, two-layer gate implementation, confusion matrix, stress test audit table |
| **Cell 9** | Experiment framework setup, config definitions, architecture table | `ExperimentConfig` dataclass, `experiment_results` DataFrame, `architectures_table` |
| **Cell 10** | Experiment 0 â€” zero-shot baseline evaluation and metric logging | `exp0_baseline` row in results table; perplexity, BLEU, ROUGE-L, BERTScore-F1, Token-F1 |
| **Cell 11** | Experiment 1 â€” QLoRA+LoRA (r=16, LR=2e-4, 300 steps) | `exp1_model`, full 6-metric evaluation, training/validation loss curves, GPU memory report |
| **Cell 12** | Experiment 2 â€” Lower learning rate (LR=5e-5) | `exp2_model`, metrics, side-by-side comparison charts with Experiment 1 |
| **Cell 13** | Experiment 3 â€” Reduced LoRA rank (r=8) | `exp3_model`, GPU memory comparison, parameter efficiency analysis |
| **Cell 14** | Experiment 4 â€” Shorter training schedule (200 steps) | `exp4_model`, compute efficiency analysis, diminishing returns chart |
| **Cell 15** | Experiment 5 â€” Stricter domain filter (â‰¥2 keyword hits) | `exp5_model`, filtering impact analysis, dataset purity vs quality trade-off report |
| **Final Dashboard** | Cross-experiment comparison, composite scoring, best model selection | Metric heatmap, radar chart, ranked experiment table, `best_model` variable assignment |
| **Results & Discussion** | Full academic narrative interpretation of experimental findings | Written analysis, improvement percentages over baseline, deployment recommendations |
| **UI Cell** | Gradio interface deployment | `temba_ui` object, public share link, live chatbot with full domain enforcement and metadata display |

---

## 14. Architecture Table

All six architectures used across the project are documented here for full reproducibility and comparative reference:

| Architecture ID | Base Model | Fine-Tuning Method | Quantization | Target Modules | Context | Notes |
|----------------|------------|-------------------|:------------:|:-------------:|:-------:|-------|
| `arch_baseline` | TinyLlama-1.1B-Chat | None (zero-shot) | 4-bit NF4 *(inference only)* | N/A â€” all frozen | 512 | Reference baseline; establishes pre-fine-tuning performance floor |
| `arch_exp1_qlora_lora` | TinyLlama-1.1B-Chat | QLoRA + LoRA | 4-bit NF4 | `q_proj`, `v_proj` | 512 | Standard configuration; best overall performance |
| `arch_exp2_qlora_lora` | TinyLlama-1.1B-Chat | QLoRA + LoRA | 4-bit NF4 | `q_proj`, `v_proj` | 512 | Lower LR variant; underfit at 300-step budget |
| `arch_exp3_qlora_lora_r8` | TinyLlama-1.1B-Chat | QLoRA + LoRA (r=8) | 4-bit NF4 | `q_proj`, `v_proj` | 512 | Memory-efficient variant; viable for edge deployment |
| `arch_exp4_qlora_lora_steps200` | TinyLlama-1.1B-Chat | QLoRA + LoRA | 4-bit NF4 | `q_proj`, `v_proj` | 512 | Short-schedule variant; 33% less compute at ~99% Exp 1 quality |
| `arch_exp5_strict_filter` | TinyLlama-1.1B-Chat | QLoRA + LoRA | 4-bit NF4 | `q_proj`, `v_proj` | 512 | Stricter domain data (â‰¥2 keyword hits); best semantic alignment |

---

## 15. How to Run

### â˜ï¸ Option A â€” Google Colab (Strongly Recommended)

Google Colab is the recommended environment because it provides a pre-configured CUDA runtime, eliminates dependency management complexity, and exactly matches the hardware (T4 GPU) used during development. All training time and memory benchmarks reported in this README were produced on Colab T4 runs.

```
Step 1 â†’ Open the notebook in Google Colab
Step 2 â†’ Runtime â†’ Change runtime type â†’ Hardware Accelerator â†’ GPU â†’ T4
Step 3 â†’ Runtime â†’ Run all  (or run cells sequentially from Cell 1 â†’ UI Cell)
Step 4 â†’ The final Gradio UI cell outputs a public share link (valid 72 hours)
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com)

### ğŸ’» Option B â€” Local Environment

```bash
# Step 1 â€” Clone or download the repository
git clone <repository-url>
cd temba-digital-bridge

# Step 2 â€” Install all required dependencies
pip install trl[peft] transformers datasets bitsandbytes accelerate
pip install evaluate bert_score rouge_score nltk
pip install gradio>=4.0.0
pip install sentence-transformers   # optional â€” required only for semantic gate (Layer 2)
pip install pandas numpy matplotlib seaborn wordcloud scikit-learn psutil

# Step 3 â€” Launch Jupyter Notebook
jupyter notebook

# Step 4 â€” Open the notebook and run all cells sequentially
#           Cell 1 â†’ Cell 2 â†’ ... â†’ UI Cell
```

> **âš ï¸ Local CUDA Requirement:** Local execution requires a CUDA-compatible GPU with at least 4 GB VRAM for 4-bit quantized inference, and at least 6 GB VRAM for QLoRA training. CPU-only execution is technically possible but significantly slower (~10â€“15Ã— longer training) and may trigger out-of-memory errors on training cells.

### âš ï¸ Critical Execution Notes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEQUENTIAL EXECUTION IS REQUIRED                                            â”‚
â”‚  The notebook is a stateful, sequential pipeline. Every later cell          â”‚
â”‚  depends on Python variables, models, and datasets produced by earlier      â”‚
â”‚  cells. Skipping cells will cause NameError or produce unexpected           â”‚
â”‚  behavior. Always run from Cell 1.                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RUNTIME DISCONNECTION RECOVERY                                              â”‚
â”‚  If the Colab runtime disconnects during training, all Python variables     â”‚
â”‚  are lost and cannot be partially restored. Re-run the notebook from        â”‚
â”‚  Cell 1. Saved model checkpoints (if configured) may be reloaded, but      â”‚
â”‚  in-memory variables must be regenerated by running preceding cells.        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TRAINING TIME EXPECTATIONS (Google Colab T4 GPU)                           â”‚
â”‚  Each experiment cell (Cells 11â€“15): ~15â€“25 minutes per experiment          â”‚
â”‚  All five experiments sequentially:  ~2 hours total                         â”‚
â”‚  For rapid testing: run only Cell 11 (Experiment 1) before the UI cell     â”‚
â”‚  The UI will use the best available trained model automatically.            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MINIMUM REQUIREMENT FOR UI LAUNCH                                           â”‚
â”‚  Cell 11 (Experiment 1) must complete successfully before the UI cell is    â”‚
â”‚  executed. The auto-model selector will fall back to the zero-shot          â”‚
â”‚  baseline model if no fine-tuned models are found in the variable           â”‚
â”‚  namespace â€” but Cell 11 completion is strongly recommended.               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 16. Dependencies

### 16.1 Full Dependency Manifest

```yaml
# â”€â”€ Core Machine Learning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
transformers:       ">= 4.36.0"   # Model loading, tokenization, training loop
trl:                ">= 0.7.0"    # SFTTrainer for supervised fine-tuning
peft:               ">= 0.7.0"    # LoRA adapter injection and management
bitsandbytes:       ">= 0.41.0"   # 4-bit NF4 quantization â€” core QLoRA
accelerate:         ">= 0.24.0"   # Distributed training and device management
datasets:           ">= 2.14.0"   # HuggingFace dataset loading and processing
torch:              ">= 2.0.0"    # PyTorch backend

# â”€â”€ Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
evaluate:           latest        # HuggingFace evaluation framework
bert_score:         latest        # BERTScore-F1 semantic similarity metric
rouge_score:        latest        # ROUGE-L LCS evaluation
nltk:               latest        # BLEU n-gram precision scoring

# â”€â”€ User Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
gradio:             ">= 4.0.0"    # Chat UI with public link sharing

# â”€â”€ Data Processing & Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pandas:             latest        # DataFrame operations and audit tables
numpy:              latest        # Numerical computing
matplotlib:         latest        # Static visualizations (loss curves, CDFs)
seaborn:            latest        # Statistical plots (KDE, heatmaps, radar)
wordcloud:          latest        # Vocabulary frequency word cloud
scikit-learn:       latest        # Normalization, cosine similarity

# â”€â”€ Optional â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sentence-transformers:  latest    # Semantic gate Layer 2 â€” query embeddings
psutil:                 latest    # System RAM and resource diagnostics
```

### 16.2 One-Line Installation

```bash
pip install trl[peft] transformers datasets bitsandbytes accelerate \
            evaluate bert_score rouge_score nltk \
            gradio \
            pandas numpy matplotlib seaborn wordcloud scikit-learn \
            sentence-transformers psutil
```

---

## 17. Rubric Coverage Map

| Rubric Requirement | Location in Project | Status |
|-------------------|---------------------|:------:|
| Domain-specific dataset curation | Cells 2â€“4; Section 4 of README | âœ… |
| Dataset size â‰¥ 1,000 samples | Cell 4 (three-stage fallback guarantee logic) | âœ… |
| Comprehensive preprocessing pipeline | Cell 3 (5-stage pipeline with per-stage audit) | âœ… |
| Tokenization justification | Cell 6 (CDF plot, `max_length=512` analysis and justification) | âœ… |
| Model architecture documentation | Cell 9 (`architectures_table`); Section 14 | âœ… |
| Model selection rationale with comparison | Section 6.3 (systematic rejection table for BERT, T5, GPT-2) | âœ… |
| Parameter-efficient fine-tuning | Cells 11â€“15 (QLoRA + LoRA across all 5 experiments) | âœ… |
| Multiple hyperparameters tuned | 5 controlled experiments: LR, LoRA rank, training steps, filter strictness | âœ… |
| â‰¥ 4 visualizations per section | EDA: Cells 2â€“4; Training: Cells 11â€“15; Dashboard: Final cell | âœ… |
| Multiple evaluation metrics | 6 metrics: BLEU, ROUGE-L, BERTScore-F1, Token-F1, Perplexity, OOD Rate | âœ… |
| Cross-experiment comparison | Final Dashboard cell with metric heatmap and radar chart | âœ… |
| â‰¥ 10% improvement over zero-shot baseline | Experiment results table + improvement percentage computation cell | âœ… |
| Domain boundary handling implementation | Cell 8 (two-layer gate); Section 11 | âœ… |
| OOD refusal rate measurement | Cells 8, 10â€“15 (refusal rate computed and logged per experiment) | âœ… |
| Qualitative OOD testing | Cell 8 (10 stress test prompts, confusion matrix, audit table) | âœ… |
| Gradio UI deployment | UI Cell (full production interface); Section 12 | âœ… |
| Radar chart visualization | Final Dashboard cell | âœ… |
| Metric heatmap visualization | Final Dashboard cell | âœ… |
| Experiment results table | Cell 9 (`experiment_results` DataFrame with all metrics) | âœ… |
| Architecture table | Cell 9 (`architectures_table`); Section 14 | âœ… |
| Rubric coverage map | This README Section 17; Cell 1 markdown header | âœ… |

---

## 18. Conclusion

The Temba Digital Bridge AI Assistant demonstrates that **parameter-efficient fine-tuning is not just a technical convenience â€” it is a democratising force**. The ability to specialise a 1.1B parameter language model for a life-critical domain, using free-tier cloud compute, in under 25 minutes, and with a peak memory footprint of 2.28 GB, fundamentally changes what is achievable by small NGOs, community health organisations, and public health agencies operating in low-resource environments.

### 18.1 Key Technical Findings

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                              â”‚
â”‚  1. EFFICIENCY â€” QLoRA + LoRA ENABLES ACCESSIBLE DOMAIN SPECIALISATION      â”‚
â”‚     A 1.1B parameter model was fully domain-adapted to WASH expertise in    â”‚
â”‚     ~24.8 minutes on a free Colab T4 GPU at only 2.28 GB peak VRAM.        â”‚
â”‚     Full fine-tuning would have required 40â€“80 GB VRAM and several hours.  â”‚
â”‚     QLoRA + LoRA makes mission-critical domain AI accessible to anyone.    â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  2. LEARNING RATE MATTERS MORE THAN TRAINING DURATION                       â”‚
â”‚     LR=2e-4 outperformed LR=5e-5 by ~7.5% perplexity within the same      â”‚
â”‚     300-step budget. Simultaneously, 200 steps achieved ~99% of 300-step   â”‚
â”‚     performance. Invest in tuning learning rate before extending training   â”‚
â”‚     duration â€” the returns on duration diminish rapidly after step 150.    â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  3. REDUCED LORA RANK IS VIABLE FOR RESOURCE-CONSTRAINED DEPLOYMENT        â”‚
â”‚     r=8 maintains competitive performance with a smaller adapter footprint  â”‚
â”‚     than r=16. WASH domain specialisation does not require high-rank        â”‚
â”‚     adapters. The task complexity fits within a compact adapter matrix â€”   â”‚
â”‚     enabling deployment on devices with extremely limited VRAM.            â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  4. DATA QUALITY IS A MEASURABLE HYPERPARAMETER                            â”‚
â”‚     Stricter domain filtering (â‰¥2 keyword hits vs â‰¥1) produced improved    â”‚
â”‚     BERTScore-F1 at the cost of slight lexical diversity reduction.         â”‚
â”‚     Preprocessing decisions have quantifiable, reproducible effects on      â”‚
â”‚     model quality. They are not just best-practices hygiene â€” they are     â”‚
â”‚     design decisions with measurable performance consequences.              â”‚
â”‚                                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  5. DOMAIN BOUNDARY ENFORCEMENT IS MEASURABLE, MANDATORY, AND EFFECTIVE    â”‚
â”‚     Programmatic OOD refusal achieved 100% success across all tested        â”‚
â”‚     out-of-domain prompts in all experiments. In health-sensitive AI,       â”‚
â”‚     a correct refusal is always safer than an uncertain generation. The     â”‚
â”‚     two-layer gate architecture is both effective and computationally       â”‚
â”‚     inexpensive â€” adding no meaningful latency to valid WASH queries.      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 18.2 Mission Alignment â€” UN SDG 6

The Temba Digital Bridge is not an academic exercise. It is a working proof of concept for how AI can accelerate progress toward **UN Sustainable Development Goal 6** â€” ensuring clean water and sanitation for all. The system provides the knowledge layer that converts installed infrastructure into genuinely accessible water safety. It makes expert-level WASH guidance available to a community health volunteer in a rural village at 3:00 AM with the same fidelity as a consultation with a specialist engineer or public health nurse.

The system is **safe**, **structured**, and **deployable** â€” and it was built to prove that responsible, mission-driven AI does not require frontier models, unlimited compute, or multi-million dollar infrastructure. It requires a clear problem, rigorous methodology, and the discipline to stay within domain.

---

## 19. References & Acknowledgements

| Resource | Role in Project |
|----------|----------------|
| [**TinyLlama/TinyLlama-1.1B-Chat-v1.0**](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) | Base model â€” chat-optimized 1.1B causal decoder-only transformer |
| [**medalpaca/medical_meadow_medical_flashcards**](https://huggingface.co/datasets/medalpaca/medical_meadow_medical_flashcards) | Clinical health training data â€” cholera, typhoid, ORS, dehydration, disease symptoms |
| [**rajpurkar/squad_v2**](https://huggingface.co/datasets/rajpurkar/squad_v2) | WASH infrastructure QA training data â€” borehole, well, chlorination, filtration |
| [**yahma/alpaca-cleaned**](https://huggingface.co/datasets/yahma/alpaca-cleaned) | General instruction-following training data â€” conversational robustness |
| [**Hugging Face `transformers`**](https://github.com/huggingface/transformers) | Model loading, tokenization, `SFTTrainer` supervised fine-tuning loop |
| [**Hugging Face `peft`**](https://github.com/huggingface/peft) | LoRA adapter configuration, injection into attention layers, adapter management |
| [**Hugging Face `trl`**](https://github.com/huggingface/trl) | `SFTTrainer` class for supervised fine-tuning with PEFT/QLoRA support |
| [**`bitsandbytes`**](https://github.com/TimDettmers/bitsandbytes) | 4-bit NF4 quantization engine â€” the computational core of QLoRA |
| [**`evaluate`**, **`bert_score`**, **`rouge_score`**](https://github.com/huggingface/evaluate) | Evaluation metrics: BLEU, ROUGE-L, BERTScore-F1, Token-Level F1 |
| [**`gradio`**](https://gradio.app) | Production UI deployment with chat interface and public link sharing |
| [**`sentence-transformers`**](https://sbert.net) | Semantic domain gate Layer 2 â€” query and anchor embedding computation |
| **Dettmers et al. (2023)** | [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) â€” quantized fine-tuning foundational methodology |
| **Hu et al. (2021)** | [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) â€” low-rank adapter theory and implementation |
| **United Nations** | [Sustainable Development Goal 6 â€” Clean Water and Sanitation](https://sdgs.un.org/goals/goal6) â€” project mission and social alignment |

---

<div align="center">

<img src="https://capsule-render.vercel.app/api?type=waving&color=0077B6,00B4D8,90E0EF&height=140&section=footer" width="100%"/>

<br/>

**Temba Digital Bridge &nbsp;|&nbsp; Holistic CleanFlow Initiative**

*Fine-tuned TinyLlama-1.1B &nbsp;Â·&nbsp; QLoRA + LoRA &nbsp;Â·&nbsp; WASH Domain Specialisation*

<br/>

[![SDG 6](https://img.shields.io/badge/ğŸŒ%20UN%20SDG%206-Clean%20Water%20%26%20Sanitation-26BDE2?style=flat-square)](https://sdgs.un.org/goals/goal6)
[![Educational](https://img.shields.io/badge/âš ï¸%20Use-Educational%20Only-orange?style=flat-square)](LICENSE)
[![Not Medical Advice](https://img.shields.io/badge/ğŸ©º%20Not%20a%20Substitute-For%20Professional%20Medical%20Advice-red?style=flat-square)](#)

<br/>

> ğŸ’§ *"Having a water point should mean having safe, sustainable water for all."*

<br/>

*Â© Temba Digital Bridge | Holistic CleanFlow â€” Educational use only.*
*This system is not a substitute for professional medical or public-health advice.*

</div>
